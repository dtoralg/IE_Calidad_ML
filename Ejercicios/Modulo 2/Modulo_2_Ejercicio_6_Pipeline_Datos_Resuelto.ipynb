{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/IE_Calidad_ML/blob/main/Ejercicios/Modulo%202/Modulo_2_Ejercicio_6_Pipeline_Datos_Resuelto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5f74aa",
      "metadata": {
        "id": "4a5f74aa"
      },
      "source": [
        "\n",
        "# **Ejercicio 6: Implementación de un Pipeline Completo de Preparación de Datos**\n",
        "## Automatización del flujo de limpieza, transformación y validación de datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce783d93",
      "metadata": {
        "id": "ce783d93"
      },
      "source": [
        "\n",
        "## Introducción\n",
        "\n",
        "En este ejercicio, construiremos un **pipeline completo de preparación de datos**, integrando todas las fases necesarias para limpiar, transformar y validar datos industriales antes de su análisis o modelado.\n",
        "\n",
        "El preprocesamiento de datos es un paso crítico en cualquier proyecto de análisis, ya que asegura que la información utilizada sea de alta calidad, sin valores atípicos, sin duplicados y correctamente normalizada.\n",
        "\n",
        "### Objetivos del ejercicio:\n",
        "- Diseñar una función para cargar datos en distintos formatos (**CSV, JSON**).\n",
        "- Implementar un **pipeline de preprocesamiento** que incluya:\n",
        "  - Eliminación de **duplicados**.\n",
        "  - Manejo de **valores nulos** mediante imputación.\n",
        "  - **Normalización** de datos numéricos.\n",
        "  - Conversión de **variables categóricas**.\n",
        "- Crear **reglas de validación** para garantizar la calidad de los datos.\n",
        "- Guardar el dataset final en distintos formatos listos para su uso posterior.\n",
        "- Evaluar la **robustez del pipeline** con distintos datasets.\n",
        "\n",
        "### Conceptos clave:\n",
        "- **Automatización del preprocesamiento**  \n",
        "- **Validación de datos**  \n",
        "- **Implementación de flujos de datos eficientes**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eaf8a47",
      "metadata": {
        "id": "6eaf8a47"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 1: Importación de librerías necesarias\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Configuración de opciones de visualización\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "print(\"Librerías importadas correctamente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e59546",
      "metadata": {
        "id": "d2e59546"
      },
      "source": [
        "\n",
        "## Descripción del Dataset\n",
        "\n",
        "Para este ejercicio, utilizaremos el dataset **\"Control de calidad en empaques de alimentos\"**, alojado en GitHub.  \n",
        "Este dataset contiene información sobre la calidad de empaques utilizados en la industria alimentaria.\n",
        "\n",
        "### **Fuente del dataset:**  \n",
        "- **CSV:** [Control de calidad en empaques de alimentos](https://raw.githubusercontent.com/dtoralg/IE_Calidad_ML/refs/heads/main/Data/control_calidad_empaques.csv)\n",
        "\n",
        "### **Estructura del dataset:**\n",
        "| Columna               | Descripción |\n",
        "|-----------------------|-------------|\n",
        "| ID_envase            | Identificador único del envase |\n",
        "| Peso_envase          | Peso del envase en gramos |\n",
        "| Espesor_material     | Espesor del material en mm |\n",
        "| Dureza_superficie    | Dureza de la superficie en N/mm² |\n",
        "| Temperatura_sellado  | Temperatura de sellado en °C |\n",
        "| Tiempo_prensado      | Tiempo de prensado en segundos |\n",
        "| Proveedor_material   | Nombre del proveedor (categórico) |\n",
        "| Tipo_envase          | Tipo de envase (categórico) |\n",
        "| Color_material       | Color del material (categórico) |\n",
        "| Defecto_detectado    | Tipo de defecto presente (multiclase) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a335a7",
      "metadata": {
        "id": "52a335a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 2: Cargar el dataset desde GitHub\n",
        "\n",
        "url_csv = \"https://raw.githubusercontent.com/dtoralg/IE_Calidad_ML/refs/heads/main/Data/control_calidad_empaques.csv\"\n",
        "df = pd.read_csv(url_csv)\n",
        "\n",
        "# Mostrar las primeras filas del dataset\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea396a8",
      "metadata": {
        "id": "2ea396a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 3: Definir una función para cargar datos en distintos formatos\n",
        "\n",
        "def cargar_datos(ruta, formato=\"csv\"):\n",
        "    \"\"\"Carga datos en formato CSV o JSON.\"\"\"\n",
        "    if formato == \"csv\":\n",
        "        return pd.read_csv(ruta)\n",
        "    elif formato == \"json\":\n",
        "        return pd.read_json(ruta)\n",
        "    else:\n",
        "        raise ValueError(\"Formato no soportado. Usa 'csv' o 'json'.\")\n",
        "\n",
        "# Cargar el dataset desde CSV usando la función\n",
        "df = cargar_datos(url_csv, formato=\"csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a858c0",
      "metadata": {
        "id": "97a858c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 4: Implementación del pipeline de preprocesamiento\n",
        "\n",
        "def preprocesar_datos(df):\n",
        "    \"\"\"Realiza limpieza, normalización y transformación de datos.\"\"\"\n",
        "\n",
        "    # Eliminar duplicados\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Manejo de valores nulos\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    columnas_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "    df[columnas_numericas] = imputer.fit_transform(df[columnas_numericas])\n",
        "\n",
        "    # Normalización de datos numéricos\n",
        "    scaler = StandardScaler()\n",
        "    df[columnas_numericas] = scaler.fit_transform(df[columnas_numericas])\n",
        "\n",
        "    # Conversión de variables categóricas con One-Hot Encoding\n",
        "    df = pd.get_dummies(df, columns=[\"Proveedor_material\", \"Tipo_envase\", \"Color_material\"], drop_first=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Aplicar el pipeline al dataset\n",
        "df_procesado = preprocesar_datos(df)\n",
        "\n",
        "print(\"Preprocesamiento completado.\")\n",
        "df_procesado.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afce573b",
      "metadata": {
        "id": "afce573b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 5: Definir reglas de validación de datos\n",
        "\n",
        "def validar_datos(df):\n",
        "    \"\"\"Verifica calidad del dataset preprocesado.\"\"\"\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        print(\"Error: Hay valores nulos en el dataset.\")\n",
        "    elif df.duplicated().sum() > 0:\n",
        "        print(\"Error: Existen registros duplicados.\")\n",
        "    else:\n",
        "        print(\"Validación superada: Los datos están limpios y listos para su uso.\")\n",
        "\n",
        "# Aplicar validación al dataset procesado\n",
        "validar_datos(df_procesado)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8eeca5",
      "metadata": {
        "id": "4f8eeca5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 6: Guardar el dataset final en distintos formatos\n",
        "\n",
        "df_procesado.to_csv(\"control_calidad_empaques_procesado.csv\", index=False)\n",
        "df_procesado.to_json(\"control_calidad_empaques_procesado.json\", orient=\"records\")\n",
        "\n",
        "print(\"Dataset guardado en formato CSV y JSON.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2add87",
      "metadata": {
        "id": "9f2add87"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 7: Evaluar la robustez del pipeline con un dataset de prueba\n",
        "\n",
        "# Crear un dataset de prueba con valores faltantes y categorías nuevas\n",
        "df_test = df.copy()\n",
        "df_test.loc[0, \"Peso_envase\"] = np.nan  # Introducir un valor nulo\n",
        "df_test.loc[1, \"Proveedor_material\"] = \"Proveedor_X\"  # Categoría nueva\n",
        "\n",
        "# Aplicar el pipeline al dataset de prueba\n",
        "df_test_procesado = preprocesar_datos(df_test)\n",
        "validar_datos(df_test_procesado)\n",
        "\n",
        "df_test_procesado.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffcea68",
      "metadata": {
        "id": "4ffcea68"
      },
      "source": [
        "\n",
        "## Conclusiones\n",
        "\n",
        "En este ejercicio hemos construido un **pipeline completo de preprocesamiento de datos**, integrando todas las fases necesarias para preparar datos industriales.\n",
        "\n",
        "### Puntos clave:\n",
        "- Se diseñó una **función modular** para cargar datos en distintos formatos.\n",
        "- Se implementó un **pipeline de limpieza**, que incluye eliminación de duplicados y manejo de valores nulos.\n",
        "- Se aplicó **normalización** a las variables numéricas para estandarizar el dataset.\n",
        "- Se convirtió información categórica en variables numéricas con **One-Hot Encoding**.\n",
        "- Se establecieron **reglas de validación** para asegurar la calidad del dataset.\n",
        "- Se evaluó la **robustez del pipeline** con datos de prueba.\n",
        "\n",
        "### Posibles mejoras:\n",
        "- Adaptar el pipeline para incluir validaciones más complejas.\n",
        "- Implementar técnicas avanzadas de imputación de valores nulos.\n",
        "- Optimizar la transformación de variables categóricas en datasets con alta cardinalidad.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}