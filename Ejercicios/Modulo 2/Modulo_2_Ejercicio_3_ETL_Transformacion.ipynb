{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/IE_Calidad_ML/blob/main/Ejercicios/Modulo%202/Modulo_2_Ejercicio_3_ETL_Transformacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f00a7701",
      "metadata": {
        "id": "f00a7701"
      },
      "source": [
        "\n",
        "# **Ejercicio 3: Integración y Transformación de Datos con ETL**\n",
        "## Aplicación de un proceso ETL para consolidación de datos de múltiples fuentes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc1ab30",
      "metadata": {
        "id": "6dc1ab30"
      },
      "source": [
        "\n",
        "## Introducción\n",
        "\n",
        "En este ejercicio, aprenderemos a realizar un **proceso ETL (Extracción, Transformación y Carga)** utilizando datos provenientes de diferentes fuentes.  \n",
        "La integración de datos es un paso crucial en cualquier pipeline de análisis, ya que permite consolidar información y prepararla para futuros análisis o modelos de Machine Learning.\n",
        "\n",
        "### Objetivos del ejercicio:\n",
        "- Cargar datasets desde distintas fuentes (CSV y JSON).\n",
        "- Unificar y combinar los datasets utilizando operaciones de **merge** y **join**.\n",
        "- Crear nuevas columnas a partir de los datos originales.\n",
        "- Convertir variables categóricas a formato numérico con **one-hot encoding** y **label encoding**.\n",
        "- Guardar el dataset transformado en un nuevo archivo para futuros análisis.\n",
        "\n",
        "### Conceptos clave:\n",
        "- Procesos ETL  \n",
        "- Integración de datos  \n",
        "- Transformación de variables  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26ea8c8",
      "metadata": {
        "id": "b26ea8c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 1: Importación de librerías necesarias\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Configuración de opciones de visualización\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "print(\"Librerías importadas correctamente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063355e9",
      "metadata": {
        "id": "063355e9"
      },
      "source": [
        "\n",
        "## Descripción de los Datos\n",
        "\n",
        "Para este ejercicio, utilizaremos dos datasets de control de calidad industrial:\n",
        "1. **Dataset principal (CSV):** \"Control de calidad en empaques de alimentos\".  \n",
        "2. **Dataset complementario (JSON):** \"Información adicional de proveedores\".  \n",
        "\n",
        "Estos datos nos permitirán realizar un proceso de integración para consolidar información relevante.\n",
        "\n",
        "### **Fuente de los datasets:**  \n",
        "- **CSV:** [Control de calidad en empaques de alimentos](https://raw.githubusercontent.com/dtoralg/IE_Calidad_ML/refs/heads/main/Data/control_calidad_empaques.csv)  \n",
        "- **JSON:** Dataset simulado con información de proveedores  \n",
        "\n",
        "### **Estructura del dataset principal (CSV):**\n",
        "| Columna               | Descripción |\n",
        "|-----------------------|-------------|\n",
        "| ID_envase            | Identificador único del envase |\n",
        "| Peso_envase          | Peso del envase en gramos |\n",
        "| Espesor_material     | Espesor del material en mm |\n",
        "| Dureza_superficie    | Dureza de la superficie en N/mm² |\n",
        "| Temperatura_sellado  | Temperatura de sellado en °C |\n",
        "| Tiempo_prensado      | Tiempo de prensado en segundos |\n",
        "| Proveedor_material   | Proveedor del material del envase (categoría) |\n",
        "| Tipo_envase          | Tipo de envase (categoría) |\n",
        "| Color_material       | Color del material (categoría) |\n",
        "| Defecto_detectado    | Tipo de defecto detectado en el envase (multiclase) |\n",
        "\n",
        "### **Estructura del dataset complementario (JSON):**\n",
        "| Columna               | Descripción |\n",
        "|-----------------------|-------------|\n",
        "| Proveedor_material   | Nombre del proveedor |\n",
        "| País_origen         | País de origen del proveedor |\n",
        "| Calidad_certificada | Indica si el proveedor tiene certificación de calidad |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41e1a6c",
      "metadata": {
        "id": "b41e1a6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 2: Cargar el dataset principal desde GitHub (CSV)\n",
        "\n",
        "url_csv = \"https://raw.githubusercontent.com/dtoralg/IE_Calidad_ML/refs/heads/main/Data/control_calidad_empaques.csv\"\n",
        "...\n",
        "\n",
        "# Mostrar las primeras filas del dataset\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eca278f",
      "metadata": {
        "id": "5eca278f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 3: Crear y cargar el dataset complementario (JSON)\n",
        "\n",
        "# Definimos un conjunto de datos ficticio en formato JSON para simular la información de proveedores\n",
        "json_data = '''\n",
        "[\n",
        "    {\"Proveedor_material\": \"Proveedor_1\", \"País_origen\": \"España\", \"Calidad_certificada\": \"Sí\"},\n",
        "    {\"Proveedor_material\": \"Proveedor_2\", \"País_origen\": \"Francia\", \"Calidad_certificada\": \"No\"},\n",
        "    {\"Proveedor_material\": \"Proveedor_3\", \"País_origen\": \"Alemania\", \"Calidad_certificada\": \"Sí\"},\n",
        "    {\"Proveedor_material\": \"Proveedor_4\", \"País_origen\": \"Italia\", \"Calidad_certificada\": \"No\"}\n",
        "]\n",
        "'''\n",
        "\n",
        "# Cargar el JSON como un DataFrame\n",
        "...\n",
        "\n",
        "# Mostrar las primeras filas del dataset\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54946dc5",
      "metadata": {
        "id": "54946dc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 4: Unificación de los datasets\n",
        "\n",
        "# Realizamos un merge entre el dataset principal y el dataset complementario en base a la columna \"Proveedor_material\"\n",
        "...\n",
        "\n",
        "# Mostrar las primeras filas del dataset unificado\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecae7c0",
      "metadata": {
        "id": "4ecae7c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 5: Creación de nuevas columnas\n",
        "\n",
        "# Crear una columna binaria llamada Calidad_certificada indicando si el proveedor tiene certificación de calidad usando una función map\n",
        "...\n",
        "\n",
        "# Extraer información adicional de \"Tipo_envase\"\n",
        "df_merged[\"Tipo_envase_num\"] = df_merged[\"Tipo_envase\"].str.extract(\"(\\d+)\").astype(float)\n",
        "\n",
        "print(\"Nuevas columnas creadas correctamente.\")\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e01a92",
      "metadata": {
        "id": "04e01a92"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 6: Transformación de variables categóricas\n",
        "\n",
        "# Aplicar Label Encoding a la columna \"País_origen\" nombrando la nueva columna \"País_origen_encoded\"\n",
        "label_encoder = ...\n",
        "...\n",
        "\n",
        "# Aplicar One-Hot Encoding a la columna \"Color_material\"\n",
        "...\n",
        "\n",
        "print(\"Transformaciones aplicadas correctamente.\")\n",
        "...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c70e4d5",
      "metadata": {
        "id": "5c70e4d5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Celda 7: Guardar el dataset transformado en un archivo CSV\n",
        "\n",
        "output_file = \"control_calidad_empaques_transformado.csv\"\n",
        "...\n",
        "\n",
        "print(f\"Dataset transformado guardado como {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f36e2ea",
      "metadata": {
        "id": "9f36e2ea"
      },
      "source": [
        "\n",
        "## Conclusiones\n",
        "\n",
        "...\n",
        "\n",
        "### Posibles mejoras:\n",
        "- Implementar validaciones para la limpieza de datos antes de la integración.\n",
        "- Aplicar estrategias de manejo de valores faltantes en datasets más complejos.\n",
        "- Explorar la normalización o escalado de variables para mejorar la preparación de datos.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}