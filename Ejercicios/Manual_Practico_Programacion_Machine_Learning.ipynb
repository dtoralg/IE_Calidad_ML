{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/IE_Calidad_ML/blob/main/Ejercicios/Manual_Practico_Programacion_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be656734",
      "metadata": {
        "id": "be656734"
      },
      "source": [
        "# Manual Práctico de Machine Learning con Python\n",
        "\n",
        "Este cuaderno contiene una guía completa para aprender y practicar los principales conceptos y técnicas de Machine Learning con Python. Está organizado por bloques temáticos. Cada bloque incluye:\n",
        "\n",
        "- Una **explicación breve**\n",
        "- Un **ejemplo ejecutable**\n",
        "- Un **ejercicio práctico guiado**\n",
        "- Una **reflexión sobre para qué sirve y cuándo se usa**\n",
        "\n",
        "> **Sigue el orden o navega por el índice según tus intereses.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7OO8G-gG9Cy3",
      "metadata": {
        "id": "7OO8G-gG9Cy3"
      },
      "source": [
        "### Consejo importante para aprender y mejorar tu programación\n",
        "\n",
        "No intentes memorizar el código.\n",
        "\n",
        "**Tu objetivo no es recordar cada función o parámetro, sino entender qué hace cada bloque**.\n",
        "\n",
        "- ¿Por qué se usa `ReLU` en una capa oculta?\n",
        "- ¿Qué significa `categorical_crossentropy` y cuándo se usa?\n",
        "- ¿Por qué usamos `softmax` en la salida para clasificación multiclase?\n",
        "\n",
        "Estas preguntas valen más que repetir código de memoria.\n",
        "\n",
        "#### Recomendaciones:\n",
        "\n",
        "- Si no recuerdas la sintaxis: **búscala o usa ejemplos anteriores**.\n",
        "- Si ves una función nueva: **lee su descripción** o prueba con `help(función)` o `Shift + Tab` si usas Jupyter Notebooks.\n",
        "- Si algo no funciona: **imprime variables, revisa formas (`.shape`), ejecuta por partes**.\n",
        "- Si te bloqueas: **comparte lo que estás intentando hacer, no solo el error**.\n",
        "\n",
        "**Aprender a programar es como aprender a hablar otro idioma: necesitas práctica, contexto y repetición. La memoria vendrá después.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1b8213",
      "metadata": {
        "id": "4a1b8213"
      },
      "source": [
        "## 1. Carga y Exploración de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482b1b60",
      "metadata": {
        "id": "482b1b60"
      },
      "source": [
        "### ¿Qué es?\n",
        "\n",
        "Cargar y explorar datos es el primer paso de cualquier proceso de análisis. Aquí es donde obtenemos una primera impresión de cómo están estructurados los datos, qué tipo de variables tenemos, si hay valores nulos, columnas irrelevantes, etc.\n",
        "\n",
        "Esto se conoce también como **ETL (Extract, Transform, Load)** en entornos más profesionales.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc50d34",
      "metadata": {
        "id": "2dc50d34"
      },
      "source": [
        "### 1.1 Cargar un archivo CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250cd5a4",
      "metadata": {
        "id": "250cd5a4"
      },
      "source": [
        "**Código de ejemplo:**  \n",
        "Cargamos un dataset desde un archivo CSV local o una URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2799c8",
      "metadata": {
        "id": "ae2799c8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar archivo desde local\n",
        "df = pd.read_csv(\"datos_calidad.csv\")  # Carga un archivo CSV en un DataFrame de pandas\n",
        "# También se puede usar una URL si el archivo está en línea\n",
        "# df = pd.read_csv(\"https://ruta-al-archivo/dataset.csv\")\n",
        "\n",
        "df.head()  # Muestra las primeras filas del DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe2c369",
      "metadata": {
        "id": "bbe2c369"
      },
      "source": [
        "**Ejercicio guiado:**  \n",
        "Cambia el nombre del archivo anterior por otro CSV disponible en tu equipo o entorno, y muestra las 10 primeras filas usando `.head(10)`.\n",
        "\n",
        "En Colab, puedes hacer click en la barra lateral izquierda, en el icono \"Archivos\" y subir ahí tus datos desde el PC o desde Drive para que estén disponibles en este entorno. Cuando reinicies el entorno desaparecerán.\n",
        "\n",
        "Una vez tengas el archivo subido, te recomiendo hacer click derecho > \"Copiar Ruta\". Por ejemplo, tendrías `pd.read_csv(\"/content/sample_data/mnist_train_small.csv\n",
        "\")`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6840d7c5",
      "metadata": {
        "id": "6840d7c5"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Este paso es fundamental para poder trabajar con datos reales. Sin esta carga inicial no es posible iniciar ningún análisis, ni visualizaciones ni modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn pone a nuestra disposición una serie de datasets muy útiles para hacer pruebas\n",
        "# https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
        "\n",
        "from sklearn.datasets import load_iris, load_breast_cancer  # Importa datasets clásicos de sklearn\n",
        "import pandas as pd  # Librería para manipular datos en formato tabla\n",
        "\n",
        "# Cargar el dataset Iris\n",
        "# Este dataset contiene 150 flores con 4 características cada una (largo/ancho de pétalo y sépalo)\n",
        "iris = load_iris()\n",
        "\n",
        "# Convertir los datos en un DataFrame de Pandas\n",
        "# - iris.data → matriz con las variables numéricas\n",
        "# - iris.feature_names → nombres de las columnas (características)\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "\n",
        "# Añadir la columna 'target' que contiene la especie (0, 1 o 2)\n",
        "df_iris[\"target\"] = iris.target\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "df_iris.head()\n"
      ],
      "metadata": {
        "id": "QTiay40PGUwo"
      },
      "id": "QTiay40PGUwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dda4e3ee",
      "metadata": {
        "id": "dda4e3ee"
      },
      "source": [
        "### 1.2 Exploración inicial del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c073c728",
      "metadata": {
        "id": "c073c728"
      },
      "outputs": [],
      "source": [
        "# Ver forma del dataset\n",
        "print(\"Dimensiones:\", df.shape)\n",
        "\n",
        "# Tipos de datos\n",
        "print(df.dtypes)\n",
        "\n",
        "# Resumen estadístico\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f438df95",
      "metadata": {
        "id": "f438df95"
      },
      "source": [
        "**Explicación:**  \n",
        "- `.shape` nos dice cuántas filas y columnas hay.\n",
        "- `.dtypes` muestra el tipo de cada columna.\n",
        "- `.describe()` da estadísticas básicas como media, desviación, mínimo y máximo para columnas numéricas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec02247",
      "metadata": {
        "id": "cec02247"
      },
      "source": [
        "### 1.3 Comprobación de valores faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c96bccb",
      "metadata": {
        "id": "8c96bccb"
      },
      "outputs": [],
      "source": [
        "# Comprobar nulos por columna\n",
        "df.isnull().sum().sort_values(ascending=False).head(10) # Nos devuelve las 10 columnas con más valores faltantes del DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ecf0c6",
      "metadata": {
        "id": "27ecf0c6"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Muchas funciones y modelos no admiten valores nulos, por lo que es importante identificarlos y decidir si se imputan (rellenan) o se eliminan.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dea959d",
      "metadata": {
        "id": "6dea959d"
      },
      "source": [
        "### 1.4 Eliminar columnas irrelevantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b476b88",
      "metadata": {
        "id": "9b476b88"
      },
      "outputs": [],
      "source": [
        "# Supongamos que hay una columna 'ID' que no aporta valor predictivo\n",
        "if 'ID' in df.columns:  # Lista los nombres de las columnas\n",
        "    df = df.drop(columns=['ID'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29752b5a",
      "metadata": {
        "id": "29752b5a"
      },
      "source": [
        "**Ejercicio guiado:**  \n",
        "Busca si tu dataset contiene columnas como identificadores, fechas de carga u observaciones constantes y elimínalas del análisis ya que no aportan capacidad predictiva y pueden inferir errores en tus modelos (relaciones espúreas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Matriz de correlación"
      ],
      "metadata": {
        "id": "4I4VN23-Dkn8"
      },
      "id": "4I4VN23-Dkn8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar las correlaciones entre variables numéricas\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Matriz de correlación entre variables numéricas\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IeLVQHnuDnM2"
      },
      "id": "IeLVQHnuDnM2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Visualizacion de la distrubución de clases"
      ],
      "metadata": {
        "id": "-SzYOAfFDtmk"
      },
      "id": "-SzYOAfFDtmk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver la cantidad de ejemplos por clase en una variable categórica (por ejemplo, 'Estado')\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(x='Estado_cod', data=df)\n",
        "plt.title(\"Distribución de clases en la variable 'Estado'\")\n",
        "plt.xlabel(\"Clase\")\n",
        "plt.ylabel(\"Cantidad de ejemplos\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_DUUIkoDDt_g"
      },
      "id": "_DUUIkoDDt_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "634a1a56",
      "metadata": {
        "id": "634a1a56"
      },
      "source": [
        "## 2. Preparación de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdadfb4",
      "metadata": {
        "id": "cfdadfb4"
      },
      "source": [
        "Antes de entrenar cualquier modelo, es fundamental preparar los datos correctamente.  \n",
        "Este proceso incluye:\n",
        "\n",
        "- **Escalar y normalizar** los datos numéricos\n",
        "- **Codificar** variables categóricas\n",
        "- Aplicar técnicas de **feature engineering**\n",
        "- Preparar los datos para evitar fugas de información y mejorar la generalización\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6394b00e",
      "metadata": {
        "id": "6394b00e"
      },
      "source": [
        "### 2.1 Escalado de variables numéricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96141985",
      "metadata": {
        "id": "96141985"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler  # Importa el escalador estándar de scikit-learn\n",
        "\n",
        "# Selección de columnas numéricas del DataFrame\n",
        "num_cols = df.select_dtypes(include='number').columns  # Detecta automáticamente las columnas numéricas\n",
        "\n",
        "# Crear el escalador:\n",
        "# - centrará las variables (media = 0)\n",
        "# - escalará para que tengan desviación estándar = 1\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Hacemos una copia del DataFrame original para mantener los datos sin modificar\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Aplicamos el escalado solo a las columnas numéricas\n",
        "# fit_transform: ajusta el escalador y transforma los datos\n",
        "df_scaled[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Visualizamos las primeras filas del DataFrame escalado\n",
        "df_scaled.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fac689a",
      "metadata": {
        "id": "2fac689a"
      },
      "source": [
        "**¿Por qué escalar?**  \n",
        "Muchos modelos (KNN, Regresión Logística, Redes Neuronales) son sensibles a la escala de las variables.  \n",
        "El `StandardScaler` transforma cada variable para que tenga media 0 y desviación estándar 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39eb68a",
      "metadata": {
        "id": "a39eb68a"
      },
      "source": [
        "### 2.2 Codificación de variables categóricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e9aef6",
      "metadata": {
        "id": "a7e9aef6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder  # Importa el codificador de etiquetas\n",
        "\n",
        "# Suponemos que 'Estado' es la variable objetivo categórica\n",
        "if 'Estado' in df.columns:  # Verificamos que la columna existe en el DataFrame\n",
        "    le = LabelEncoder()  # Crea una instancia del codificador LabelEncoder\n",
        "    df['Estado_cod'] = le.fit_transform(df['Estado'])  # Codifica las categorías como números enteros\n",
        "    print(\"Clases:\", list(le.classes_))  # Muestra las clases originales asignadas a cada número\n",
        "    df[['Estado', 'Estado_cod']].head()  # Muestra una vista de las etiquetas originales y su codificación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad832cd7",
      "metadata": {
        "id": "ad832cd7"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Muchos modelos de machine learning necesitan que las variables categóricas estén en formato numérico.  \n",
        "`LabelEncoder` convierte etiquetas como `\"OK\"`, `\"KO\"` en `0`, `1`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd2a304",
      "metadata": {
        "id": "ecd2a304"
      },
      "source": [
        "### 2.3 Ingeniería de características (Feature Engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5b3ac1",
      "metadata": {
        "id": "7d5b3ac1"
      },
      "outputs": [],
      "source": [
        "# Crear una nueva variable: relación entre temperatura y presión\n",
        "df['Temp_Pres_ratio'] = df['Temperatura'] / df['Presion'] # Creamos una nueva variable en df lamada Temp_Pres_Ratio\n",
        "df[['Temperatura', 'Presion', 'Temp_Pres_ratio']].head()  # Muestra las primeras filas del DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222ae148",
      "metadata": {
        "id": "222ae148"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "A veces, relaciones entre variables aportan más valor predictivo que las variables originales por separado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fa988d0",
      "metadata": {
        "id": "1fa988d0"
      },
      "source": [
        "### 2.4 Eliminar columnas constantes o duplicadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610a6119",
      "metadata": {
        "id": "610a6119"
      },
      "outputs": [],
      "source": [
        "# Eliminar columnas que tienen un único valor (constantes)\n",
        "for col in df.columns:  # Recorre cada columna del DataFrame\n",
        "    if df[col].nunique() == 1:  # Si solo tiene un valor único (sin variabilidad)\n",
        "        df = df.drop(columns=[col])  # Elimina esa columna del DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e9bbc0",
      "metadata": {
        "id": "b2e9bbc0"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Las columnas que no cambian no aportan información y pueden dificultar el entrenamiento o inflar el tamaño del modelo innecesariamente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Codificación de variables categóricas (OneHotEncoder)"
      ],
      "metadata": {
        "id": "b3M-El7XC3Kd"
      },
      "id": "b3M-El7XC3Kd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir variables categóricas en columnas binarias (0/1)\n",
        "df_ohe = pd.get_dummies(df, columns=['Categoria'], drop_first=True)  # 'Categoria' debe reemplazarse por tu columna categórica\n",
        "df_ohe.head()"
      ],
      "metadata": {
        "id": "iaZBDvmeC3e_"
      },
      "id": "iaZBDvmeC3e_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Para qué sirve?**  \n",
        "- Convierte **variables categóricas** (como \"Color\", \"Estado\", \"Tipo\") en **columnas numéricas binarias** (0/1), que los modelos pueden entender.\n",
        "- Cada categoría se convierte en una nueva columna.\n",
        "- Permite usar **textos como entradas** en modelos de machine learning sin errores."
      ],
      "metadata": {
        "id": "YzOdBIxhC_VE"
      },
      "id": "YzOdBIxhC_VE"
    },
    {
      "cell_type": "markdown",
      "id": "a9ab26d4",
      "metadata": {
        "id": "a9ab26d4"
      },
      "source": [
        "## 3. Modelos Supervisados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0b241d",
      "metadata": {
        "id": "ef0b241d"
      },
      "source": [
        "### 3.1 Regresión Lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698420bf",
      "metadata": {
        "id": "698420bf"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression  # Importa el modelo de regresión lineal\n",
        "\n",
        "# Seleccionamos una sola variable como predictor: 'Temperatura'\n",
        "# y una variable objetivo continua: 'Presion'\n",
        "X_simple = df[[\"Temperatura\"]]  # Debe ser un DataFrame (matriz 2D)\n",
        "y_simple = df[\"Presion\"]        # Variable de salida (1D)\n",
        "\n",
        "# Crear el modelo de regresión lineal\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Entrenar el modelo ajustando una recta (y = a*x + b) a los datos\n",
        "model_lr.fit(X_simple, y_simple)\n",
        "\n",
        "# Predecir los valores de presión a partir de la temperatura\n",
        "# Las predicciones estarán en la misma escala que y_simple\n",
        "y_pred_lr = model_lr.predict(X_simple)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a9fc9b",
      "metadata": {
        "id": "35a9fc9b"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "La regresión lineal permite predecir un valor numérico continuo.  \n",
        "Es útil como modelo de referencia y también para interpretar relaciones lineales entre variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35b1560",
      "metadata": {
        "id": "e35b1560"
      },
      "source": [
        "### 3.2 Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9246c53a",
      "metadata": {
        "id": "9246c53a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression  # Importa el modelo de regresión logística\n",
        "\n",
        "# Usamos como entrada las columnas numéricas escaladas\n",
        "# X_log: variables predictoras\n",
        "# y_log: variable objetivo binaria (0 o 1)\n",
        "X_log = df_scaled[num_cols]\n",
        "y_log = df[\"Estado_cod\"]\n",
        "\n",
        "# Crear el modelo de regresión logística\n",
        "# - max_iter=1000: número máximo de iteraciones para que el algoritmo converja\n",
        "model_log = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Entrenar el modelo con los datos completos\n",
        "# El modelo ajusta una función sigmoide para predecir la probabilidad de clase 1\n",
        "model_log.fit(X_log, y_log)\n",
        "\n",
        "# Hacer predicciones sobre los mismos datos (en práctica, usarías X_test)\n",
        "# El resultado es una clase predicha (0 o 1)\n",
        "y_pred_log = model_log.predict(X_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2368aa44",
      "metadata": {
        "id": "2368aa44"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Clasifica instancias en dos clases (binaria) de forma simple pero efectiva.  \n",
        "Suele ser el modelo base para comparar con otros más complejos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd57382",
      "metadata": {
        "id": "fdd57382"
      },
      "source": [
        "### 3.3 Árbol de Decisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400c383a",
      "metadata": {
        "id": "400c383a"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier  # Importa el modelo de árbol de decisión\n",
        "\n",
        "# Crear el modelo:\n",
        "# - max_depth=4: limita la profundidad máxima del árbol a 4 niveles (para evitar sobreajuste)\n",
        "model_dt = DecisionTreeClassifier(max_depth=4)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "# El árbol aprende reglas de decisión (si... entonces...) a partir de los datos\n",
        "model_dt.fit(X_train_log, y_train_log)\n",
        "\n",
        "# Realizar predicciones sobre los datos de prueba\n",
        "# El modelo recorre las ramas del árbol para clasificar cada muestra\n",
        "y_pred_dt = model_dt.predict(X_test_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebd3eac7",
      "metadata": {
        "id": "ebd3eac7"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Los árboles son fáciles de interpretar y permiten entender reglas de decisión.  \n",
        "Son útiles cuando hay relaciones no lineales entre las variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d78e33ae",
      "metadata": {
        "id": "d78e33ae"
      },
      "source": [
        "### 3.4 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233a72c4",
      "metadata": {
        "id": "233a72c4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier  # Importa el clasificador Random Forest\n",
        "\n",
        "# Crear el modelo:\n",
        "# - n_estimators=100: usa 100 árboles de decisión independientes\n",
        "# - random_state=42: fija la semilla para que los resultados sean reproducibles\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "# Cada árbol se entrena con una muestra aleatoria distinta del dataset (bagging)\n",
        "model_rf.fit(X_train_log, y_train_log)\n",
        "\n",
        "# Realizar predicciones sobre los datos de prueba\n",
        "# El resultado final es el voto mayoritario entre los árboles del bosque\n",
        "y_pred_rf = model_rf.predict(X_test_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d685b0a4",
      "metadata": {
        "id": "d685b0a4"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Es un ensemble de árboles que mejora la robustez y generalización del modelo.  \n",
        "Muy usado en la industria por su rendimiento y facilidad de uso.\n",
        "\n",
        "Cuando el modelo lo permite, es recomendable fijar `random_state` para que el resultado sea repetible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la importancia de cada variable es muy útil en modelos basados en árboles (como Random Forest)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear una serie de Pandas con la importancia de cada variable\n",
        "# model_rf.feature_importances_ devuelve un array con un valor por cada variable de entrada\n",
        "# Usamos los nombres de las columnas como índice\n",
        "importancia = pd.Series(model_rf.feature_importances_, index=X_train_log.columns)\n",
        "\n",
        "# Ordenamos las variables por importancia y creamos un gráfico de barras horizontal\n",
        "importancia.sort_values().plot(kind='barh')\n",
        "\n",
        "# Personalizamos el gráfico\n",
        "plt.title(\"Importancia de cada variable (Random Forest)\")\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.grid(True)\n",
        "plt.show()  # Muestra el gráfico generado\n"
      ],
      "metadata": {
        "id": "QyosLIw6GD8Z"
      },
      "id": "QyosLIw6GD8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cde069ec",
      "metadata": {
        "id": "cde069ec"
      },
      "source": [
        "### 3.5 K-Nearest Neighbors (KNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c7380e",
      "metadata": {
        "id": "81c7380e"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier  # Importa el clasificador KNN\n",
        "\n",
        "# Crear el modelo KNN:\n",
        "# - n_neighbors=5: el modelo clasificará en función de los 5 vecinos más cercanos\n",
        "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "# El modelo memoriza todos los puntos de entrenamiento (no ajusta parámetros)\n",
        "model_knn.fit(X_train_log, y_train_log)\n",
        "\n",
        "# Hacer predicciones sobre los datos de prueba\n",
        "# Para cada punto nuevo, el modelo mira sus 5 vecinos más cercanos y predice la clase más frecuente\n",
        "y_pred_knn = model_knn.predict(X_test_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5be8d25",
      "metadata": {
        "id": "d5be8d25"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Clasifica en base a los vecinos más cercanos.  \n",
        "Muy intuitivo y efectivo en datasets pequeños y bien escalados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2c5c30",
      "metadata": {
        "id": "9d2c5c30"
      },
      "source": [
        "## 4. Modelos No Supervisados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ae3297",
      "metadata": {
        "id": "61ae3297"
      },
      "source": [
        "Los modelos no supervisados aprenden a partir de datos **sin etiquetas**. Se utilizan para:\n",
        "- Explorar la estructura interna de los datos\n",
        "- Agrupar observaciones similares\n",
        "- Reducir la dimensionalidad para visualización o mejora de modelos supervisados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bba2a6c",
      "metadata": {
        "id": "8bba2a6c"
      },
      "source": [
        "### 4.1 K-Means - Agrupamiento no supervisado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f2478d",
      "metadata": {
        "id": "01f2478d"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans  # Importa el algoritmo K-Means\n",
        "\n",
        "# Aplicar KMeans con 2 grupos (clusters)\n",
        "# - n_clusters=2: número de agrupaciones a encontrar\n",
        "# - random_state=42: para que el resultado sea reproducible\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "\n",
        "# Ajusta el modelo y predice el cluster asignado para cada muestra\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualizar los clusters si X tiene solo 2 variables (2D)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='Set2')  # Pinta cada punto según el cluster asignado\n",
        "\n",
        "# Añadir los centroides (centros de los clusters) como marcadores 'X' negros\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='black', s=200, alpha=0.7, marker='X', label='Centroides')\n",
        "\n",
        "# Personalizar el gráfico\n",
        "plt.title(\"Clusters detectados por K-Means\")\n",
        "plt.xlabel(\"Variable 1\")\n",
        "plt.ylabel(\"Variable 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2dc249",
      "metadata": {
        "id": "8e2dc249"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "K-Means permite **agrupar observaciones similares** sin necesidad de etiquetas.  \n",
        "Se usa en segmentación de clientes, análisis exploratorio y detección de patrones no etiquetados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "343d1695",
      "metadata": {
        "id": "343d1695"
      },
      "source": [
        "### 4.1 PCA - Análisis de Componentes Principales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737ee445",
      "metadata": {
        "id": "737ee445"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA  # Importa el modelo de análisis de componentes principales\n",
        "import matplotlib.pyplot as plt        # Librería para graficar\n",
        "\n",
        "# Aplicar PCA para reducir los datos a 2 dimensiones (componentes principales)\n",
        "# Esto nos permite visualizar datos con muchas variables en un plano 2D\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Ajusta el PCA sobre X_log y transforma los datos a 2 componentes\n",
        "X_pca = pca.fit_transform(X_log)\n",
        "\n",
        "# Crear un gráfico de dispersión con las nuevas dimensiones\n",
        "# c=y_log → colorea cada punto según su clase original (si hay etiquetas disponibles)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_log, cmap='coolwarm', alpha=0.7)\n",
        "\n",
        "# Personalizar el gráfico\n",
        "plt.xlabel(\"Componente 1\")\n",
        "plt.ylabel(\"Componente 2\")\n",
        "plt.title(\"Visualización PCA (2D)\")\n",
        "plt.grid(True)\n",
        "plt.show()  # Muestra el gráfico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3284e9",
      "metadata": {
        "id": "7a3284e9"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "PCA transforma las variables originales en un nuevo conjunto de variables **no correlacionadas**, que capturan la mayor parte de la varianza.\n",
        "Se usa para visualización, compresión o preprocesamiento antes de modelos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d315c7ba",
      "metadata": {
        "id": "d315c7ba"
      },
      "source": [
        "## 5. Evaluación de Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa1c410b",
      "metadata": {
        "id": "fa1c410b"
      },
      "source": [
        "Evaluar el rendimiento de un modelo es tan importante como entrenarlo.  \n",
        "Dependiendo del tipo de problema (clasificación o regresión), usaremos diferentes métricas:\n",
        "\n",
        "- Clasificación: precisión, recall, F1, matriz de confusión, ROC, AUC\n",
        "- Regresión: MAE, MSE, RMSE, R²\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f07e9a8",
      "metadata": {
        "id": "8f07e9a8"
      },
      "source": [
        "### 5.1 Matriz de Confusión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dafff74",
      "metadata": {
        "id": "9dafff74"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Importa funciones para crear y mostrar la matriz de confusión\n",
        "\n",
        "# Calcula la matriz de confusión a partir de los valores reales y los predichos\n",
        "cm = confusion_matrix(y_test_log, y_pred_rf)\n",
        "\n",
        "# Crea el objeto de visualización a partir de la matriz\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "# Muestra la matriz con un mapa de colores azul\n",
        "disp.plot(cmap='Blues')\n",
        "\n",
        "# Añade un título y muestra el gráfico\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f721e8",
      "metadata": {
        "id": "25f721e8"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Muestra el número de aciertos y errores para cada clase.  \n",
        "Ideal para saber **qué clases se confunden entre sí**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be835dc8",
      "metadata": {
        "id": "be835dc8"
      },
      "source": [
        "### 5.2 Precisión, Recall, F1 Score, Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fad1a1",
      "metadata": {
        "id": "05fad1a1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # Importa métricas de evaluación\n",
        "\n",
        "# Accuracy: porcentaje de predicciones correctas (aciertos totales)\n",
        "print(\"Accuracy:\", accuracy_score(y_test_log, y_pred_rf))\n",
        "\n",
        "# Precisión: de todas las veces que se predijo clase positiva, ¿cuántas eran correctas?\n",
        "print(\"Precisión:\", precision_score(y_test_log, y_pred_rf))\n",
        "\n",
        "# Recall: de todas las verdaderas instancias positivas, ¿cuántas detectamos?\n",
        "print(\"Recall:\", recall_score(y_test_log, y_pred_rf))\n",
        "\n",
        "# F1 Score: media armónica entre precisión y recall (balance entre ambos)\n",
        "print(\"F1 Score:\", f1_score(y_test_log, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc36c6d",
      "metadata": {
        "id": "6cc36c6d"
      },
      "source": [
        "**¿Cuándo usar cada una?**\n",
        "- `Precisión`: cuántos de los positivos predichos eran correctos\n",
        "- `Recall`: cuántos de los positivos reales fueron capturados\n",
        "- `F1`: equilibrio entre precisión y recall\n",
        "- `Accuracy`: proporción de aciertos totales (menos útil si hay clases desbalanceadas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9a280f",
      "metadata": {
        "id": "af9a280f"
      },
      "source": [
        "### 5.3 Curva ROC y AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e7becd",
      "metadata": {
        "id": "b1e7becd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc  # Importa funciones para calcular curva ROC y el área bajo la curva (AUC)\n",
        "\n",
        "# Obtener las probabilidades predichas para la clase positiva (índice 1)\n",
        "# predict_proba devuelve [P(clase 0), P(clase 1)], nos interesa la de clase 1\n",
        "y_prob = model_rf.predict_proba(X_test_log)[:, 1]\n",
        "\n",
        "# Calcular los puntos de la curva ROC:\n",
        "# - fpr: tasa de falsos positivos\n",
        "# - tpr: tasa de verdaderos positivos\n",
        "# - thresholds: umbrales usados para clasificar\n",
        "fpr, tpr, thresholds = roc_curve(y_test_log, y_prob)\n",
        "\n",
        "# Calcular el área bajo la curva ROC (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Dibujar la curva ROC\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")  # Curva ROC\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')  # Línea diagonal (modelo aleatorio)\n",
        "\n",
        "# Etiquetas del gráfico\n",
        "plt.xlabel(\"Tasa de falsos positivos (FPR)\")\n",
        "plt.ylabel(\"Tasa de verdaderos positivos (TPR)\")\n",
        "plt.title(\"Curva ROC\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()  # Muestra el gráfico generado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780c265c",
      "metadata": {
        "id": "780c265c"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Mide la capacidad del modelo para distinguir entre clases.  \n",
        "Cuanto más se acerque el AUC a 1, mejor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d98aa3",
      "metadata": {
        "id": "88d98aa3"
      },
      "source": [
        "### 5.4 MAE, MSE, RMSE, R²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3103018",
      "metadata": {
        "id": "c3103018"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Importa métricas de regresión\n",
        "import numpy as np  # Importa numpy para calcular la raíz cuadrada\n",
        "\n",
        "# Evaluamos el modelo de regresión lineal previamente entrenado\n",
        "\n",
        "# MAE (Mean Absolute Error): promedio de los errores absolutos\n",
        "mae = mean_absolute_error(y_simple, y_pred_lr)\n",
        "\n",
        "# MSE (Mean Squared Error): promedio de los errores al cuadrado\n",
        "mse = mean_squared_error(y_simple, y_pred_lr)\n",
        "\n",
        "# RMSE: raíz cuadrada del MSE (misma unidad que la variable original)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# R² (Coeficiente de determinación): qué proporción de la varianza de y es explicada por el modelo\n",
        "r2 = r2_score(y_simple, y_pred_lr)\n",
        "\n",
        "# Mostrar los resultados con dos decimales\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd424e6",
      "metadata": {
        "id": "6fd424e6"
      },
      "source": [
        "**¿Qué mide cada métrica?**\n",
        "- `MAE`: error absoluto medio\n",
        "- `MSE`: error cuadrático medio (penaliza más errores grandes)\n",
        "- `RMSE`: raíz cuadrada del MSE (en mismas unidades que la variable)\n",
        "- `R²`: porcentaje de variabilidad explicada por el modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452b71ea",
      "metadata": {
        "id": "452b71ea"
      },
      "source": [
        "## 6. Técnicas de Optimización y Regularización"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2479e070",
      "metadata": {
        "id": "2479e070"
      },
      "source": [
        "Estas técnicas ayudan a mejorar el rendimiento y la generalización del modelo, especialmente cuando los datos son complejos o limitados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfedb75b",
      "metadata": {
        "id": "bfedb75b"
      },
      "source": [
        "### 6.1 Regularización L1 y L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93fe5ef",
      "metadata": {
        "id": "b93fe5ef"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression  # Importa el modelo de regresión logística\n",
        "\n",
        "# Modelo con regularización L1 (Lasso):\n",
        "# - penalty='l1': aplica regularización L1 (algunos coeficientes se vuelven exactamente cero)\n",
        "# - solver='liblinear': necesario para usar L1 en problemas pequeños o binarios\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model_l1.fit(X_train_log, y_train_log)  # Entrena el modelo con regularización L1\n",
        "\n",
        "# Modelo con regularización L2 (Ridge):\n",
        "# - penalty='l2': aplica regularización L2 (reduce la magnitud de los coeficientes)\n",
        "# - solver='lbfgs' se usa por defecto y es compatible con L2\n",
        "model_l2 = LogisticRegression(penalty='l2')\n",
        "model_l2.fit(X_train_log, y_train_log)  # Entrena el modelo con regularización L2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fb7664",
      "metadata": {
        "id": "72fb7664"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "- `L1`: fuerza coeficientes a cero (selección de variables)\n",
        "- `L2`: reduce la magnitud de los coeficientes (reduce overfitting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6048e62",
      "metadata": {
        "id": "b6048e62"
      },
      "source": [
        "### 6.2 Búsqueda de hiperparámetros con GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f050f14",
      "metadata": {
        "id": "4f050f14"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV  # Importa GridSearchCV para optimización de hiperparámetros\n",
        "\n",
        "# Definir la rejilla de hiperparámetros que queremos probar:\n",
        "# - 'n_estimators': número de árboles en el bosque\n",
        "# - 'max_depth': profundidad máxima de cada árbol\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5, 10]\n",
        "}\n",
        "\n",
        "# Crear la búsqueda con validación cruzada:\n",
        "# - modelo base: RandomForestClassifier\n",
        "# - param_grid: rejilla de combinaciones a probar\n",
        "# - cv=5: validación cruzada con 5 particiones\n",
        "# - scoring='f1_macro': métrica a optimizar (media del F1 por clase)\n",
        "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1_macro')\n",
        "\n",
        "# Ajustar la búsqueda al conjunto de entrenamiento\n",
        "grid.fit(X_train_log, y_train_log)\n",
        "\n",
        "# Mostrar los mejores parámetros encontrados\n",
        "print(\"Mejores parámetros:\", grid.best_params_)\n",
        "\n",
        "# Mostrar el mejor resultado promedio de F1 (macro) durante la validación cruzada\n",
        "print(\"Mejor score F1 Macro:\", grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe99e0a6",
      "metadata": {
        "id": "fe99e0a6"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Permite probar muchas combinaciones de hiperparámetros y seleccionar la mejor automáticamente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1083a6",
      "metadata": {
        "id": "ae1083a6"
      },
      "source": [
        "### 6.3 RandomizedSearchCV (alternativa rápida a GridSearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac4f17c3",
      "metadata": {
        "id": "ac4f17c3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV  # Búsqueda aleatoria de hiperparámetros con validación cruzada\n",
        "from scipy.stats import randint  # Distribución de enteros aleatorios\n",
        "\n",
        "# Definir el espacio de búsqueda para los hiperparámetros:\n",
        "# - 'n_estimators': número de árboles (valores entre 50 y 150)\n",
        "# - 'max_depth': profundidad del árbol (valores entre 3 y 10)\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 150),\n",
        "    'max_depth': randint(3, 10)\n",
        "}\n",
        "\n",
        "# Crear la búsqueda aleatoria:\n",
        "# - modelo base: RandomForestClassifier\n",
        "# - param_distributions: espacio de búsqueda definido arriba\n",
        "# - n_iter=10: probará 10 combinaciones aleatorias\n",
        "# - cv=5: validación cruzada con 5 particiones\n",
        "# - random_state: para obtener resultados reproducibles\n",
        "random_search = RandomizedSearchCV(RandomForestClassifier(),\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=10,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "\n",
        "# Entrenar el modelo y buscar la mejor combinación\n",
        "random_search.fit(X_train_log, y_train_log)\n",
        "\n",
        "# Mostrar la mejor combinación encontrada\n",
        "print(\"Mejores parámetros:\", random_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993c9dbb",
      "metadata": {
        "id": "993c9dbb"
      },
      "source": [
        "### 6.4 Rebalanceo de clases con SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e47f600",
      "metadata": {
        "id": "9e47f600"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE  # Importa la técnica SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "from collections import Counter  # Permite contar la frecuencia de valores en listas\n",
        "\n",
        "# Crear el objeto SMOTE:\n",
        "# - random_state=42: fija la semilla para reproducibilidad\n",
        "# SMOTE genera ejemplos sintéticos nuevos para la clase minoritaria\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "# fit_resample ajusta y aplica el sobremuestreo\n",
        "X_train_sm, y_train_sm = smote.fit_resample(X_train_log, y_train_log)\n",
        "\n",
        "# Mostrar la distribución de clases antes y después del rebalanceo\n",
        "print(\"Distribución original:\", Counter(y_train_log))\n",
        "print(\"Distribución balanceada:\", Counter(y_train_sm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4e1b72",
      "metadata": {
        "id": "fe4e1b72"
      },
      "source": [
        "**¿Para qué sirve?**  \n",
        "Genera muestras sintéticas para la clase minoritaria y evita el sobreajuste a la clase mayoritaria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086a07bf",
      "metadata": {
        "id": "086a07bf"
      },
      "source": [
        "## 6. Redes Neuronales: desde MLP hasta LSTM y CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f62ea953",
      "metadata": {
        "id": "f62ea953"
      },
      "source": [
        "Las redes neuronales permiten modelar relaciones no lineales complejas.  \n",
        "En esta sección las construiremos de forma progresiva:\n",
        "\n",
        "1. Red Neuronal Multicapa básica (MLP)\n",
        "2. MLP más profunda con Dropout y activación ReLU\n",
        "3. Red LSTM (para series temporales)\n",
        "4. Red Convolucional (CNN, para datos con estructura espacial)\n",
        "5. Uso de EarlyStopping para evitar overfitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7051c557",
      "metadata": {
        "id": "7051c557"
      },
      "source": [
        "### 6.1 Red Neuronal Multicapa (MLP) Básica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BG4EuTLk3SBa",
      "metadata": {
        "id": "BG4EuTLk3SBa"
      },
      "source": [
        "#### Funciones de activación más comunes para MLP\n",
        "\n",
        "**ReLU (`relu`)**  \n",
        "- f(x) = max(0, x)  \n",
        "- Muy usada en capas ocultas  \n",
        "- Rápida y evita saturación\n",
        "\n",
        "**Sigmoid (`sigmoid`)**  \n",
        "- f(x) = 1 / (1 + exp(-x))  \n",
        "- Salida entre 0 y 1  \n",
        "- Útil en clasificación binaria (salida)\n",
        "\n",
        "**Softmax (`softmax`)**  \n",
        "- Devuelve probabilidades que suman 1  \n",
        "- Usada en clasificación multiclase (salida)\n",
        "\n",
        "**Lineal (`linear`)**  \n",
        "- f(x) = x  \n",
        "- Se usa en regresión (salida continua)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jXpomz9u13Se",
      "metadata": {
        "id": "jXpomz9u13Se"
      },
      "source": [
        "### 6.1.1 Red Neuronal Multicapa para Regresión\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NJdAiZFV47vk",
      "metadata": {
        "id": "NJdAiZFV47vk"
      },
      "source": [
        "#### Funciones de pérdida más comunes para regresión\n",
        "\n",
        "- **`mean_squared_error` (MSE)**  \n",
        "  Penaliza fuertemente los errores grandes. Es la más usada por defecto.  \n",
        "  `model.compile(optimizer='adam', loss='mean_squared_error')`\n",
        "\n",
        "- **`mean_absolute_error` (MAE)**  \n",
        "  Más robusta a outliers. Penaliza todos los errores por igual.  \n",
        "  `model.compile(optimizer='adam', loss='mean_absolute_error')`\n",
        "\n",
        "- **`huber_loss`**  \n",
        "  Combina MSE y MAE: usa MSE para errores pequeños y MAE para errores grandes.  \n",
        "  `model.compile(optimizer='adam', loss='huber_loss')`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kv60OInD2Dsh",
      "metadata": {
        "id": "Kv60OInD2Dsh"
      },
      "source": [
        "Este bloque muestra cómo usar una red neuronal para predecir valores numéricos continuos (por ejemplo: precio, temperatura, producción, etc.), en lugar de clasificar clases.\n",
        "\n",
        "· Última capa: solo tiene 1 neurona\n",
        "\n",
        "· Activación final: sin activación (linear)\n",
        "\n",
        "· Función de pérdida: usamos mean_squared_error en lugar de categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0R7JzOuT2T8S",
      "metadata": {
        "id": "0R7JzOuT2T8S"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense  # Capa densamente conectada (fully connected), típica en MLP\n",
        "\n",
        "# Crear un modelo secuencial: las capas se añaden en orden, una tras otra\n",
        "model_reg = Sequential()\n",
        "\n",
        "# Primera capa oculta con 64 neuronas y activación ReLU\n",
        "# input_shape define el número de variables de entrada\n",
        "model_reg.add(Dense(64, activation='relu', input_shape=(X_train_reg.shape[1],)))\n",
        "\n",
        "# Segunda capa oculta con 32 neuronas y activación ReLU\n",
        "model_reg.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Capa de salida con 1 sola neurona (predicción escalar continua)\n",
        "# No se usa función de activación → salida lineal (ideal para regresión)\n",
        "model_reg.add(Dense(1))\n",
        "\n",
        "# Compilar el modelo:\n",
        "# - Optimizador: Adam, muy usado por su estabilidad y buen rendimiento\n",
        "# - Pérdida: mean_squared_error (error cuadrático medio, típico en regresión)\n",
        "# - Métrica: mean_absolute_error para tener una idea clara del error medio en unidades originales\n",
        "model_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo:\n",
        "# - validation_split=0.2: reserva el 20% de los datos de entrenamiento para validación interna\n",
        "# - epochs=100: número de pasadas completas sobre los datos\n",
        "# - batch_size=32: número de muestras que se procesan antes de actualizar los pesos\n",
        "# - verbose=0: no muestra el progreso durante el entrenamiento (usa 1 o 2 si se quiere visualizar)\n",
        "history_reg = model_reg.fit(X_train_reg, y_train_reg,\n",
        "                            validation_split=0.2,\n",
        "                            epochs=100,\n",
        "                            batch_size=32,\n",
        "                            verbose=0)\n",
        "\n",
        "print(\"Entrenamiento completado.\")\n",
        "\n",
        "# Predecimos los valores continuos sobre los datos de test\n",
        "y_pred_reg = model_reg.predict(X_test_reg)  # Devuelve un array de valores predichos\n",
        "\n",
        "print(\"Primeras predicciones:\", np.round(y_pred_reg[:5].flatten(), 2))  # Redondeamos para visualización\n",
        "print(\"Valores reales:\", np.round(y_test_reg[:5].values, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AEg_8h-o17hB",
      "metadata": {
        "id": "AEg_8h-o17hB"
      },
      "source": [
        "### 6.1.2 Red Neuronal Multicapa para Clasificación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HtkSMmtY5GHf",
      "metadata": {
        "id": "HtkSMmtY5GHf"
      },
      "source": [
        "#### Funciones de pérdida más comunes en clasificación\n",
        "\n",
        "**`categorical_crossentropy`**  \n",
        "- Para clasificación multiclase con one-hot encoding  \n",
        "- Se usa cuando la salida tiene varias clases y está codificada como vector  \n",
        "- `model.compile(optimizer='adam', loss='categorical_crossentropy')`\n",
        "\n",
        "**`sparse_categorical_crossentropy`**  \n",
        "- Igual que `categorical_crossentropy`, pero con etiquetas como enteros en lugar de one-hot  \n",
        "- Más cómoda si no usas `to_categorical()`  \n",
        "- `model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bbe6864",
      "metadata": {
        "id": "6bbe6864"
      },
      "source": [
        "**Código explicado paso a paso:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b871e8",
      "metadata": {
        "id": "53b871e8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense  # Crea una capa densa (fully connected)\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Codificamos las etiquetas enteras como vectores one-hot (necesario para usar softmax + categorical_crossentropy)\n",
        "y_train_cat = to_categorical(y_train_log)\n",
        "y_test_cat = to_categorical(y_test_log)\n",
        "\n",
        "# Crear el modelo secuencial: se añaden las capas en orden, una tras otra\n",
        "model_basic = Sequential()\n",
        "\n",
        "# Primera (y única) capa oculta con 16 neuronas y activación ReLU\n",
        "# input_shape define el número de características de entrada\n",
        "model_basic.add(Dense(16, activation='relu', input_shape=(X_train_log.shape[1],)))\n",
        "\n",
        "# Capa de salida con 2 neuronas (porque tenemos 2 clases) y activación softmax\n",
        "# Softmax convierte las salidas en probabilidades que suman 1\n",
        "model_basic.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compilar el modelo:\n",
        "# - Optimizador Adam ajusta automáticamente los pesos\n",
        "# - categorical_crossentropy porque usamos one-hot encoding\n",
        "# - Métrica de evaluación: accuracy\n",
        "model_basic.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "# - validation_split=0.2: usa el 20% de los datos para validación\n",
        "# - epochs=100: número de veces que se recorren todos los datos de entrenamiento\n",
        "# - batch_size=32: número de muestras usadas antes de actualizar los pesos\n",
        "# - verbose=0: silencia la salida (puedes usar 1 o 2 para ver el progreso)\n",
        "history_basic = model_basic.fit(X_train_log, y_train_cat,\n",
        "                                validation_split=0.2,\n",
        "                                epochs=100,\n",
        "                                batch_size=32,\n",
        "                                verbose=0)\n",
        "\n",
        "print(\"Entrenamiento terminado.\")\n",
        "\n",
        "# Predecimos las probabilidades para cada clase\n",
        "y_pred_prob = model_basic.predict(X_test_log)  # Devuelve una probabilidad por clase para cada muestra\n",
        "\n",
        "# Convertimos las probabilidades en etiquetas predichas (la clase con mayor probabilidad)\n",
        "y_pred_class = np.argmax(y_pred_prob, axis=1)  # Selecciona el índice (clase) con mayor probabilidad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f28cf002",
      "metadata": {
        "id": "f28cf002"
      },
      "source": [
        "**¿Qué hace este modelo?**\n",
        "\n",
        "- Toma las variables como entrada (X)\n",
        "- Usa una capa con 16 neuronas y activación ReLU\n",
        "- Devuelve una predicción por clase (softmax)\n",
        "\n",
        "Puedes probar aumentando las neuronas o cambiando el optimizador (el mas comun es `adam`) para encontrar el mejor resultado."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para los casos de clasificación multiclase debemos usar el classification_report\n",
        "# adaptado para reflejar en él todas las clases.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# y_test_mc e y_pred_mc deberían ser etiquetas de clasificación multiclase\n",
        "# Reemplaza con tus propias variables si usas otro modelo\n",
        "# digits = 3 indica que tenemos 3 clases\n",
        "print(classification_report(y_test_log, y_pred_log, digits=3))\n"
      ],
      "metadata": {
        "id": "U7WJ-QVOFr1v"
      },
      "id": "U7WJ-QVOFr1v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.3 Red Neuronal Multicapa para Clasificación Binaria"
      ],
      "metadata": {
        "id": "GD85-M3gIR6g"
      },
      "id": "GD85-M3gIR6g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Función de pérdida más comun en clasificación binaria\n",
        "\n",
        "**`binary_crossentropy`**  \n",
        "- Para clasificación binaria (2 clases)  \n",
        "- Calcula el error entre la clase real y la probabilidad predicha  \n",
        "- Recuerda usar la funcion de activación `sigmoid` en la capa de salida\n",
        "- `model.compile(optimizer='adam', loss='binary_crossentropy')`"
      ],
      "metadata": {
        "id": "g8aOTP73Ifgq"
      },
      "id": "g8aOTP73Ifgq"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense  # Capa densamente conectada (fully connected)\n",
        "\n",
        "# Crear el modelo secuencial: se añaden las capas en orden\n",
        "model_bin = Sequential()\n",
        "\n",
        "# Primera capa oculta con 16 neuronas y activación ReLU\n",
        "# input_shape define el número de características de entrada\n",
        "model_bin.add(Dense(16, activation='relu', input_shape=(X_train_log.shape[1],)))\n",
        "\n",
        "# Capa de salida con 1 neurona y activación sigmoid\n",
        "# La salida será un valor entre 0 y 1 (probabilidad de clase 1)\n",
        "model_bin.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar el modelo:\n",
        "# - binary_crossentropy porque es clasificación binaria\n",
        "# - Métrica de evaluación: accuracy\n",
        "model_bin.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "# - validation_split=0.2: usa el 20% de los datos como validación\n",
        "# - epochs=100: número de iteraciones completas sobre los datos\n",
        "# - batch_size=32: número de muestras procesadas por paso\n",
        "# - verbose=0: silencia la salida (usa 1 o 2 para verla)\n",
        "history_bin = model_bin.fit(X_train_log, y_train_log,\n",
        "                            validation_split=0.2,\n",
        "                            epochs=100,\n",
        "                            batch_size=32,\n",
        "                            verbose=0)\n",
        "\n",
        "print(\"Entrenamiento terminado.\")\n",
        "\n",
        "# .predict() devuelve la probabilidad de que la clase sea 1\n",
        "y_pred_prob_bin = model_bin.predict(X_test_log)\n",
        "\n",
        "# Aplicamos un umbral de 0.5 para convertir la probabilidad en clase (0 o 1)\n",
        "# Podemos ajustar este umbral si tenemos datos desbalanceados (0.45, 0.3, ...)\n",
        "y_pred_class_bin = (y_pred_prob_bin > 0.5).astype(int)\n",
        "\n",
        "# Mostramos las primeras predicciones\n",
        "print(\"Predicciones:\", y_pred_class_bin[:5].flatten())\n"
      ],
      "metadata": {
        "id": "e0PoAJW6IYV0"
      },
      "id": "e0PoAJW6IYV0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bfee04fe",
      "metadata": {
        "id": "bfee04fe"
      },
      "source": [
        "### 6.2 MLP profunda con Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253dd965",
      "metadata": {
        "id": "253dd965"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Crear modelo secuencial (las capas se apilan en orden)\n",
        "model_deep = Sequential()\n",
        "\n",
        "# Primera capa densa con 64 neuronas y activación ReLU\n",
        "# input_shape indica el número de variables de entrada (características)\n",
        "model_deep.add(Dense(64, activation='relu', input_shape=(X_train_log.shape[1],)))\n",
        "\n",
        "# Dropout apaga aleatoriamente el 30% de las neuronas durante el entrenamiento\n",
        "# Esto ayuda a prevenir overfitting (que el modelo memorice demasiado los datos)\n",
        "model_deep.add(Dropout(0.3))\n",
        "\n",
        "# Segunda capa oculta con 32 neuronas y activación ReLU\n",
        "model_deep.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Capa de salida con softmax para clasificación multiclase\n",
        "# El número de neuronas depende del número de clases (columnas de y_train_cat)\n",
        "model_deep.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
        "\n",
        "# Compilar el modelo:\n",
        "# - optimizador: Adam (ajusta los pesos automáticamente)\n",
        "# - función de pérdida: categorical_crossentropy (clasificación multiclase con one-hot)\n",
        "# - métrica: accuracy (porcentaje de aciertos)\n",
        "model_deep.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo:\n",
        "# - validation_split: usa el 20% de los datos como validación\n",
        "# - epochs: número de iteraciones completas sobre el conjunto de datos\n",
        "# - verbose=0: no muestra la salida del entrenamiento (se puede usar 1 o 2 para ver el progreso)\n",
        "history_deep = model_deep.fit(X_train_log, y_train_cat, validation_split=0.2, epochs=50, verbose=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc05272",
      "metadata": {
        "id": "fdc05272"
      },
      "source": [
        "**¿Qué mejora esta red respecto a la anterior?**\n",
        "\n",
        "- Tiene más capas y más neuronas.\n",
        "- `Dropout` apaga aleatoriamente neuronas durante el entrenamiento (evita que el modelo memorice demasiado).\n",
        "- Es más robusta y generaliza mejor. Reduce el riesgo de overfitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d45ad9",
      "metadata": {
        "id": "d3d45ad9"
      },
      "source": [
        "### 6.3 LSTM - Red para Series Temporales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed35c7a",
      "metadata": {
        "id": "bed35c7a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Simulamos datos secuenciales: 100 muestras, cada una con 10 pasos de tiempo y 5 variables por paso\n",
        "X_seq = np.random.rand(100, 10, 5)  # Forma: (n_muestras, pasos_temporales, n_variables)\n",
        "\n",
        "# Etiquetas aleatorias binarias (0 o 1), codificadas como one-hot para clasificación binaria\n",
        "y_seq = to_categorical(np.random.randint(0, 2, 100))\n",
        "\n",
        "# Crear modelo secuencial\n",
        "model_lstm = Sequential()  # Modelo lineal: capa tras capa en orden\n",
        "\n",
        "# Añadir una capa LSTM con 32 neuronas\n",
        "# Esta capa recibe secuencias de 10 pasos con 5 características cada uno\n",
        "model_lstm.add(LSTM(32, input_shape=(10, 5)))\n",
        "\n",
        "# Capa de salida con softmax para clasificación binaria (2 clases → one-hot)\n",
        "model_lstm.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compilar el modelo usando función de pérdida multiclase\n",
        "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo con los datos secuenciales simulados\n",
        "model_lstm.fit(X_seq, y_seq, epochs=10, verbose=0)\n",
        "\n",
        "print(\"Modelo LSTM entrenado para datos secuenciales.\")\n",
        "\n",
        "# Realizar predicciones de probabilidad por clase\n",
        "y_pred_lstm_prob = model_lstm.predict(X_seq)  # X_seq debe tener forma (muestras, pasos_tiempo, variables)\n",
        "\n",
        "# Convertir probabilidades en etiquetas de clase (la más probable)\n",
        "y_pred_lstm_class = np.argmax(y_pred_lstm_prob, axis=1)\n",
        "\n",
        "# Mostrar predicciones y clases reales\n",
        "print(\"Predicciones (clase más probable):\", y_pred_lstm_class[:5])\n",
        "print(\"Clases reales:\", np.argmax(y_seq, axis=1)[:5])  # Si y_seq está en formato one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e2d15b",
      "metadata": {
        "id": "65e2d15b"
      },
      "source": [
        "**¿Cuándo usar LSTM?**\n",
        "\n",
        "- Cuando tienes secuencias temporales: sensores en el tiempo, texto, series temporales\n",
        "- El modelo \"recuerda\" estados anteriores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2776a0b",
      "metadata": {
        "id": "b2776a0b"
      },
      "source": [
        "### 6.4 CNN - Red Convolucional para Imágenes o Datos Espaciales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db01565",
      "metadata": {
        "id": "6db01565"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Simular imágenes (100 muestras de 28x28 píxeles con 1 canal → imágenes escala de grises)\n",
        "X_img = np.random.rand(100, 28, 28, 1)\n",
        "y_img = to_categorical(np.random.randint(0, 3, 100))  # Crea etiquetas aleatorias en 3 clases (codificadas one-hot)\n",
        "\n",
        "# Crear el modelo secuencial\n",
        "model_cnn = Sequential()  # Modelo secuencial: las capas se añaden en orden lineal\n",
        "\n",
        "# Capa convolucional 2D: extrae características espaciales (bordes, texturas, etc.)\n",
        "model_cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "# Capa de max pooling: reduce la dimensionalidad espacial manteniendo lo más relevante\n",
        "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Capa Flatten: convierte la salida 2D anterior en un vector 1D para conectarla a capas densas\n",
        "model_cnn.add(Flatten())\n",
        "\n",
        "# Capa oculta totalmente conectada (fully connected)\n",
        "model_cnn.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Capa de salida con softmax: devuelve una probabilidad para cada una de las 3 clases\n",
        "model_cnn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compilar el modelo con pérdida para clasificación multiclase\n",
        "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo con los datos simulados\n",
        "model_cnn.fit(X_img, y_img, epochs=10, verbose=0)\n",
        "\n",
        "print(\"Modelo CNN entrenado para datos con estructura espacial.\")\n",
        "\n",
        "# Predecimos las probabilidades por clase para cada imagen\n",
        "# X_img debe tener forma (n_muestras, alto, ancho, canales)\n",
        "y_pred_cnn_prob = model_cnn.predict(X_img)  # Devuelve una probabilidad por clase para cada muestra\n",
        "\n",
        "# Convertimos las probabilidades en etiquetas predichas (la clase con mayor probabilidad)\n",
        "y_pred_cnn_class = np.argmax(y_pred_cnn_prob, axis=1)\n",
        "\n",
        "# Mostrar predicciones y clases reales\n",
        "print(\"Predicciones (clase más probable):\", y_pred_cnn_class[:5])\n",
        "print(\"Clases reales:\", np.argmax(y_img, axis=1)[:5])  # Si y_img está en formato one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cc7b36",
      "metadata": {
        "id": "87cc7b36"
      },
      "source": [
        "**¿Para qué sirve?**\n",
        "\n",
        "- Para imágenes, series espectrales, o cualquier dato con estructura bidimensional\n",
        "- Detecta patrones locales con filtros convolucionales\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6033dc07",
      "metadata": {
        "id": "6033dc07"
      },
      "source": [
        "### 6.5 Uso de EarlyStopping para evitar overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf3fa7b",
      "metadata": {
        "id": "dcf3fa7b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # Detiene el entrenamiento si no mejora en validación\n",
        "\n",
        "# Crear un callback de EarlyStopping:\n",
        "# - monitor: métrica que se vigila (val_loss = pérdida en el conjunto de validación)\n",
        "# - patience: nº de épocas que se permite sin mejora antes de detener el entrenamiento\n",
        "# - restore_best_weights: restaura los pesos del modelo con mejor rendimiento\n",
        "early_stop = EarlyStopping(monitor='val_loss',\n",
        "                           patience=5,\n",
        "                           restore_best_weights=True)\n",
        "\n",
        "# Definir un modelo secuencial simple con 2 capas ocultas y softmax en salida\n",
        "model_es = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_log.shape[1],)),  # Primera capa oculta\n",
        "    Dense(32, activation='relu'),                                       # Segunda capa oculta\n",
        "    Dense(y_train_cat.shape[1], activation='softmax')                   # Capa de salida (multiclase)\n",
        "])\n",
        "\n",
        "# Compilar el modelo:\n",
        "# - Optimizador: Adam\n",
        "# - Pérdida: categorical_crossentropy (porque estamos usando codificación one-hot)\n",
        "# - Métrica: accuracy\n",
        "model_es.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo:\n",
        "# - validation_split=0.2: usa el 20% para validación interna\n",
        "# - epochs=100: máximo de épocas (EarlyStopping puede detener antes)\n",
        "# - batch_size=32: muestras usadas por iteración\n",
        "# - callbacks: ejecuta EarlyStopping si no mejora la pérdida de validación\n",
        "# - verbose=0: no imprime el progreso (usa 1 si quieres ver más)\n",
        "model_es.fit(X_train_log, y_train_cat,\n",
        "             validation_split=0.2,\n",
        "             epochs=100,\n",
        "             batch_size=32,\n",
        "             callbacks=[early_stop],\n",
        "             verbose=0)\n",
        "\n",
        "print(\"Entrenamiento con EarlyStopping finalizado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2674248",
      "metadata": {
        "id": "b2674248"
      },
      "source": [
        "**¿Qué hace EarlyStopping?**\n",
        "\n",
        "- Supervisa el rendimiento en validación\n",
        "- Si no mejora durante varias épocas (`patience`), detiene el entrenamiento\n",
        "- Ahorra tiempo y evita sobreajuste\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6 Visualización de la pérdida durante el entrenamiento"
      ],
      "metadata": {
        "id": "3DJ105s4CUMV"
      },
      "id": "3DJ105s4CUMV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este código podemos graficar de manera sencilla la pérdida durante el entrenamiento tanto en train como en val. Esto nos ayuda a identificar cuando el modelo está cayendo en  overfitting e identificar como el modelo está mejorando."
      ],
      "metadata": {
        "id": "zHL_l6WfCdrr"
      },
      "id": "zHL_l6WfCdrr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar cómo evoluciona la pérdida durante el entrenamiento\n",
        "plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Validación')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.title('Evolución de la pérdida durante el entrenamiento')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fdfQryHMCZCC"
      },
      "id": "fdfQryHMCZCC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1d8ddc3b",
      "metadata": {
        "id": "1d8ddc3b"
      },
      "source": [
        "## 7. Buenas prácticas de programación y consejos para aprender Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab5fdda",
      "metadata": {
        "id": "aab5fdda"
      },
      "source": [
        "Aprender a programar y resolver problemas de Machine Learning lleva tiempo. Estos consejos te ayudarán a desarrollar buenos hábitos desde el inicio y evitar errores comunes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b5a6ba",
      "metadata": {
        "id": "e1b5a6ba"
      },
      "source": [
        "### 7.1 Estructura y claridad en tu código"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed74d838",
      "metadata": {
        "id": "ed74d838"
      },
      "source": [
        "- Escribe tu código en bloques lógicos separados por secciones.\n",
        "- Usa nombres de variables descriptivos (`X_train`, `y_test`, `modelo_rf`, etc).\n",
        "- Deja comentarios claros explicando qué hace cada bloque.\n",
        "- Elimina código muerto o duplicado que no se esté usando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53e4d1f",
      "metadata": {
        "id": "b53e4d1f"
      },
      "source": [
        "### 7.2 Prueba tu código por partes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce2f6aa",
      "metadata": {
        "id": "2ce2f6aa"
      },
      "source": [
        "- No intentes resolver todo en una sola celda o paso.\n",
        "- Ejecuta paso a paso: primero carga de datos, luego separación, luego modelo...\n",
        "- Si algo falla, imprime los shapes de tus variables y revisa el contenido con `.head()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9bb4934",
      "metadata": {
        "id": "b9bb4934"
      },
      "source": [
        "### 7.3 Aprende a leer documentación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc64ff0c",
      "metadata": {
        "id": "bc64ff0c"
      },
      "source": [
        "- Usa `Shift + Tab` en notebooks o `help(función)` para ver qué hace un método.\n",
        "- Lee la documentación oficial de Scikit-learn, Pandas, Seaborn y Keras.\n",
        "- Copia ejemplos pequeños y prueba cambiando parámetros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f231896f",
      "metadata": {
        "id": "f231896f"
      },
      "source": [
        "### 7.4 Reutiliza tus propios códigos y este manual como plantillas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa2de43",
      "metadata": {
        "id": "5aa2de43"
      },
      "source": [
        "- Guarda notebooks que ya te han funcionado.\n",
        "- Usa una plantilla para cargar datos, otra para entrenar modelos, otra para gráficas.\n",
        "- Esto te permitirá resolver ejercicios más rápido en el futuro y te dará confianza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3313142b",
      "metadata": {
        "id": "3313142b"
      },
      "source": [
        "### 7.5 Ten paciencia y repite lo básico muchas veces"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed990b9",
      "metadata": {
        "id": "9ed990b9"
      },
      "source": [
        "- No intentes dominar todos los modelos a la vez.\n",
        "- Domina primero uno (RandomForest o Regresión Logística) y evalúalo bien.\n",
        "- El aprendizaje real viene de repetir un problema con distintos datasets y estructuras.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}